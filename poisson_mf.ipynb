{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gamma\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sparse\n",
    "import recometrics\n",
    "from utils import convert_to_index_lst, batch_mapk\n",
    "import hpfrec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_poisson_matrix(latent_factor_size, n_users, n_items, a, a_prime, b_prime, c, c_prime, d_prime, weighting_array=None):\n",
    "    \"\"\"\n",
    "    Generate a Poisson matrix with the given parameters.\n",
    "    \"\"\"\n",
    "    if weighting_array is None:\n",
    "        weighting_array = np.ones((latent_factor_size))\n",
    "    user_factors = np.empty((0, latent_factor_size))\n",
    "    item_factors = np.empty((0, latent_factor_size))\n",
    "    for _ in range(n_users):\n",
    "        activity = gamma.rvs(a=a_prime, scale=a_prime / b_prime)\n",
    "        prefs = gamma.rvs(a=a, scale=activity, size=latent_factor_size)\n",
    "        user_factors = np.vstack((user_factors, prefs))\n",
    "\n",
    "    for _ in range(n_items):\n",
    "        popularity = gamma.rvs(a=c_prime, scale=c_prime / d_prime)\n",
    "        prefs = gamma.rvs(a=c, scale=popularity, size=latent_factor_size)\n",
    "        item_factors = np.vstack((item_factors, prefs))\n",
    "    print(user_factors)\n",
    "    print(item_factors)\n",
    "\n",
    "    item_scores_weighted = (user_factors * weighting_array) @ item_factors.T\n",
    "    item_scores_unweighted = user_factors @ item_factors.T\n",
    "    return item_scores_weighted, item_scores_unweighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_factor_size = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighting_array = np.array([-1] * 10 + [1] * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.66320726e-03 4.34633090e-03 1.58059317e-08 ... 1.01006098e-02\n",
      "  9.01220555e-07 1.12906053e-02]\n",
      " [2.50132428e-01 9.68952006e-02 2.15599535e-07 ... 8.13223007e-02\n",
      "  4.72316378e-05 5.03075853e-01]\n",
      " [1.19770350e-03 4.18694589e-03 4.65460129e-04 ... 7.44402238e-09\n",
      "  2.09056886e-02 1.30026807e-02]\n",
      " ...\n",
      " [3.04023098e-04 6.77889577e-08 9.81581321e-05 ... 1.84853577e-06\n",
      "  4.11460562e-03 1.10647850e-05]\n",
      " [1.29358978e-01 3.35038509e-05 1.10012372e-04 ... 1.39284015e-04\n",
      "  1.21571385e-02 1.31987640e-05]\n",
      " [4.78529201e-02 2.37249350e-02 4.53257824e-02 ... 1.77734068e-06\n",
      "  1.17028461e-02 8.54580019e-02]]\n",
      "[[1.85657409e-06 3.85041288e-08 1.45587821e-04 ... 1.39110801e-04\n",
      "  9.08937500e-04 2.10123393e-06]\n",
      " [3.35496228e-06 6.44352412e-03 1.60539300e-02 ... 5.41133605e-02\n",
      "  4.42876008e-03 8.53142165e-03]\n",
      " [1.22717445e-04 6.90257637e-03 3.94671517e-03 ... 3.25826838e-05\n",
      "  9.98831184e-04 8.87310048e-04]\n",
      " ...\n",
      " [6.03357317e-04 3.42819846e-03 3.00267899e-02 ... 1.06492884e-01\n",
      "  8.36872698e-05 2.81391999e-02]\n",
      " [8.79693352e-03 4.90068825e-09 3.00281460e-01 ... 2.41716809e-03\n",
      "  5.83615451e-04 1.21208383e-06]\n",
      " [1.95553949e-01 4.69937094e-04 9.87441697e-02 ... 1.02660002e-02\n",
      "  8.55300131e-03 1.47993924e-01]]\n"
     ]
    }
   ],
   "source": [
    "item_scores_weighted, item_scores_unweighted = generate_poisson_matrix(latent_factor_size=latent_factor_size, n_users=1000, n_items=1000, a=0.3, a_prime=0.3, b_prime=1, c=0.3, c_prime=0.3, d_prime=1, weighting_array=weighting_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-6.491745970722523, 8.025739875979186, 0.0661880329643494)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_scores_weighted.min(), item_scores_weighted.max(), item_scores_weighted.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.505055174571272e-31, 10.015659212733654, 0.09847930341445094)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_scores_unweighted.min(), item_scores_unweighted.max(), item_scores_unweighted.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181902"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(item_scores_unweighted > 0.01).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92730"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(item_scores_weighted > 0.01).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "uw_cutoff = 0.01\n",
    "w_cutoff = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_lst = []\n",
    "for i in range(len(item_scores_unweighted)):\n",
    "    proxy_pref = np.where(item_scores_unweighted[i] > uw_cutoff)[0]\n",
    "    if len(proxy_pref) < 10:\n",
    "        del_lst.append(i)\n",
    "item_scores_unweighted = np.delete(item_scores_unweighted, del_lst, axis=0)\n",
    "item_scores_weighted = np.delete(item_scores_weighted, del_lst, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((633, 1000), (633, 1000))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_scores_unweighted.shape, item_scores_weighted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_watch_matrix = sparse.csr_matrix((item_scores_unweighted > uw_cutoff).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use recometrics to create train/test split\n",
    "X_train, X_test = recometrics.split_reco_train_test(proxy_watch_matrix, split_type=\"all\", items_test_fraction=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMENTED OUT BECAUSE CAN TAKE DIRECT COO ARRAY\n",
    "\n",
    "# Convert X_test to np array and then index list. \n",
    "# Generate true prefs\n",
    "# Convert X_train to dataframe \n",
    "\n",
    "\n",
    "\n",
    "X_train_arr = X_train.toarray()\n",
    "concat_lst = []\n",
    "for i in range(X_train_arr.shape[0]):\n",
    "    for j in range(X_train_arr.shape[1]):\n",
    "        if X_train_arr[i][j] > 0:\n",
    "            concat_lst.append(pd.DataFrame({\"UserId\": [i], \"ItemId\": [j], \"Count\": [X_train_arr[i][j]]}))\n",
    "X_train_df = pd.concat(concat_lst)\n",
    "X_train_df\n",
    "\n",
    "item_set = X_train_df['ItemId'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[78, 224, 267, 278, 301, 327, 377, 753, 980]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proxy_prefs = convert_to_index_lst(X_test.toarray())\n",
    "proxy_prefs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[212, 224, 327, 370, 489, 555, 597, 623, 707, 753, 807]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_prefs = []\n",
    "for i in range(item_scores_weighted.shape[0]):\n",
    "    true_pref = np.where(item_scores_weighted[i] > w_cutoff)[0] #FIXME: Try turning this into 0.001 if no results still.\n",
    "    true_prefs.append(list(true_pref))\n",
    "true_prefs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether strictly subset/this is a potential hypothesis for why no observed curve.\n",
    "\n",
    "#FIXME: Get this to compare with entire set.\n",
    "\n",
    "# count = 0\n",
    "# for i in range(len(proxy_prefs)):\n",
    "#     if len(np.setdiff1d(np.array(true_prefs[i]), np.array(proxy_prefs[i]))) > 0:\n",
    "#         count += 1\n",
    "# count        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_losses, true_losses = [], []\n",
    "def validation_hook(solver):\n",
    "    recommendations = [solver.topN(i) for i in range(item_scores_weighted.shape[0])]\n",
    "\n",
    "    # PROXY PREFS AND TRUE PREFS MUST BE A LISt OF INDEXES\n",
    "    global proxy_prefs\n",
    "    global true_prefs\n",
    "    proxy_loss, true_loss = batch_mapk(recommendations, proxy_prefs, true_prefs)\n",
    "    proxy_losses.append(proxy_loss)\n",
    "    true_losses.append(true_loss)\n",
    "    # Calculate validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_coo = X_train.tocoo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: This isn't finishing running. Hypothesis: matrix has too many sparse columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis: Has to do with my changes. Try first with original code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************\n",
      "Hierarchical Poisson Factorization\n",
      "**********************************\n",
      "\n",
      "Number of users: 633\n",
      "Number of items: 746\n",
      "Latent factors to use: 30\n",
      "\n",
      "Initializing parameters...\n",
      "Allocating Phi matrix...\n",
      "Initializing optimization procedure...\n"
     ]
    }
   ],
   "source": [
    "recommender = hpfrec.HPF()\n",
    "recommender.fit(X_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "HPF.fit() got an unexpected keyword argument 'callback'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m recommender \u001b[39m=\u001b[39m hpfrec\u001b[39m.\u001b[39mHPF()\n\u001b[0;32m----> 2\u001b[0m recommender\u001b[39m.\u001b[39;49mfit(X_train_coo, callback\u001b[39m=\u001b[39;49mvalidation_hook)\n",
      "\u001b[0;31mTypeError\u001b[0m: HPF.fit() got an unexpected keyword argument 'callback'"
     ]
    }
   ],
   "source": [
    "recommender = hpfrec.HPF()\n",
    "recommender.fit(X_train_coo, callback=validation_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "HPF.fit() missing 1 required positional argument: 'counts_df'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[177], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m hpfrec\u001b[39m.\u001b[39;49mHPF()\u001b[39m.\u001b[39;49mfit()\n",
      "\u001b[0;31mTypeError\u001b[0m: HPF.fit() missing 1 required positional argument: 'counts_df'"
     ]
    }
   ],
   "source": [
    "proxy_losses, true_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys-HtsDc_SN-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
